# Modern Opensource Lakehouse: DuckDB + Apache Iceberg + MinIO + dbt

LaboratÃ³rio prÃ¡tico de um **Lakehouse moderno e 100% open source** totalmente containerizado, demonstrando conceitos avanÃ§ados de engenharia de dados como versionamento, time travel, schema evolution e transformaÃ§Ãµes com dbt.

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#visÃ£o-geral)
- [Arquitetura](#arquitetura)
- [Conceitos](#conceitos)
- [PrÃ©-requisitos](#prÃ©-requisitos)
- [InstalaÃ§Ã£o e Uso](#instalaÃ§Ã£o-e-uso)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [Funcionalidades](#funcionalidades)
- [Acessando os ServiÃ§os](#acessando-os-serviÃ§os)
- [Notebooks Jupyter](#notebooks-jupyter)
- [Exemplos de Uso](#exemplos-de-uso)
- [ComparaÃ§Ã£o com Databricks](#comparaÃ§Ã£o-com-databricks)

## ğŸ¯ VisÃ£o Geral

Este projeto implementa um **Lakehouse 100% open source** completo em ambiente local usando:

- **DuckDB**: Engine analÃ­tico in-memory otimizado para OLAP
- **Apache Iceberg**: Tabela format para versionamento e time travel
- **MinIO**: Storage S3-compatible para simular cloud storage
- **dbt**: Ferramenta de transformaÃ§Ã£o de dados (ELT)
- **Jupyter Lab**: Ambiente de notebooks integrado para anÃ¡lise e exploraÃ§Ã£o
- **Docker Compose**: OrquestraÃ§Ã£o de todos os serviÃ§os

### O que Ã© um Lakehouse?

Um **Lakehouse** combina as melhores caracterÃ­sticas de um **Data Lake** (armazenamento barato, formatos abertos) com as de um **Data Warehouse** (ACID transactions, schema enforcement, performance).

**Vantagens:**
- âœ… Armazenamento econÃ´mico (formato Parquet/Delta/Iceberg)
- âœ… Suporte a dados estruturados, semi-estruturados e nÃ£o estruturados
- âœ… ACID transactions e versionamento
- âœ… Time travel (acessar versÃµes anteriores dos dados)
- âœ… Schema evolution (evoluir schema sem quebrar compatibilidade)
- âœ… Performance de queries analÃ­ticas
- âœ… IntegraÃ§Ã£o com ferramentas modernas (Spark, dbt, etc)

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Docker Compose                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚    MinIO     â”‚      â”‚    DuckDB    â”‚      â”‚     dbt      â”‚  â”‚
â”‚  â”‚  (S3 Local)  â”‚â—„â”€â”€â”€â”€â”€â”¤  (Analytics) â”‚â—„â”€â”€â”€â”€â”€â”¤ (Transform)  â”‚  â”‚
â”‚  â”‚  Port: 9000  â”‚      â”‚              â”‚      â”‚              â”‚  â”‚
â”‚  â”‚  Port: 9001  â”‚      â”‚              â”‚      â”‚              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â–²                    â–²                    â–²            â”‚
â”‚         â”‚                    â”‚                    â”‚            â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                              â”‚                                 â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                    â”‚   Jupyter    â”‚                            â”‚
â”‚                    â”‚   (Notebooks)â”‚                            â”‚
â”‚                    â”‚  Port: 8888  â”‚                            â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                            â–²                                   â”‚
â”‚                            â”‚                                   â”‚
â”‚        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚              Iceberg Tables                                    â”‚
â”‚         (s3://lakehouse/iceberg/)                              â”‚
â”‚                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Init Service (One-time)                     â”‚  â”‚
â”‚  â”‚  â€¢ Cria bucket                                           â”‚  â”‚
â”‚  â”‚  â€¢ Gera dados fake                                       â”‚  â”‚
â”‚  â”‚  â€¢ Cria tabela Iceberg                                   â”‚  â”‚
â”‚  â”‚  â€¢ Insere dados                                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Dados

1. **InicializaÃ§Ã£o**: O serviÃ§o `init` cria o bucket no MinIO, gera dados fake e cria a tabela Iceberg
2. **Armazenamento**: Dados sÃ£o armazenados no MinIO (S3-compatible) em formato Iceberg
3. **AnÃ¡lise**: DuckDB consulta diretamente as tabelas Iceberg no MinIO
4. **TransformaÃ§Ã£o**: dbt transforma os dados brutos em modelos analÃ­ticos (marts)

## ğŸ“š Conceitos

### Apache Iceberg

**Apache Iceberg** Ã© uma especificaÃ§Ã£o de tabela aberta para analytics em data lakes. Ele fornece:

- **ACID Transactions**: Garante consistÃªncia dos dados
- **Time Travel**: Acesse versÃµes anteriores dos dados
- **Schema Evolution**: Adicione/remova colunas sem quebrar queries antigas
- **Hidden Partitioning**: Particionamento automÃ¡tico e otimizado
- **Metadata Management**: Metadados versionados e eficientes

#### ImplementaÃ§Ã£o no Projeto

Este projeto implementa **Iceberg real** usando:

1. **Tabela DuckDB** (`vendas_iceberg`): Tabela persistente no DuckDB para uso imediato
2. **Tabela Iceberg Real** (`s3://lakehouse/iceberg/vendas_real/`): Estrutura Iceberg completa com:
   - Metadados versionados (`metadata/*.metadata.json`)
   - Arquivos de dados Parquet (`data/*.parquet`)
   - Snapshots para time travel
   - Estrutura de diretÃ³rios compatÃ­vel com Iceberg

**Script disponÃ­vel:**
- `create_real_iceberg_table.py`: Cria tabela Iceberg REAL com metadados completos

ğŸ“– **Para mais detalhes tÃ©cnicos sobre a implementaÃ§Ã£o do Iceberg, consulte:** [`scripts/ICEBERG_IMPLEMENTATION.md`](scripts/ICEBERG_IMPLEMENTATION.md)

**Exemplo de Time Travel:**
```sql
-- Ver dados de 1 hora atrÃ¡s
SELECT * FROM vendas_iceberg 
FOR TIMESTAMP AS OF '2024-01-01 10:00:00';

-- Ver snapshot especÃ­fico
SELECT * FROM vendas_iceberg 
FOR VERSION AS OF 5;
```

**Exemplo de Schema Evolution:**
```sql
-- Adicionar nova coluna sem quebrar queries antigas
ALTER TABLE vendas_iceberg 
ADD COLUMN novo_campo VARCHAR;
```

### DuckDB

**DuckDB** Ã© um banco de dados analÃ­tico in-memory otimizado para OLAP (Online Analytical Processing). CaracterÃ­sticas:

- âš¡ Performance excepcional para queries analÃ­ticas
- ğŸ”Œ IntegraÃ§Ã£o nativa com Parquet, CSV, JSON
- ğŸ“¦ ExtensÃµes para S3, Iceberg, Postgres, etc
- ğŸ IntegraÃ§Ã£o Python/R fÃ¡cil
- ğŸ’¾ Zero configuraÃ§Ã£o

### MinIO

**MinIO** Ã© um servidor de armazenamento de objetos S3-compatible. Usado aqui para simular cloud storage localmente.

## ğŸš€ PrÃ©-requisitos

- **Docker** (versÃ£o 20.10+)
- **Docker Compose** (versÃ£o 2.0+)
- **Git** (para clonar o repositÃ³rio)

## ğŸ“¦ InstalaÃ§Ã£o e Uso

### 1. Clone o repositÃ³rio

```bash
git clone <repo-url>
cd modern-lakehouse-duckdb-iceberg
```

### 2. Inicie os serviÃ§os

```bash
docker compose up
```

**âš ï¸ Importante**: Execute sem `-d` na primeira vez para ver os logs em tempo real e garantir que tudo estÃ¡ funcionando.

Este comando irÃ¡ automaticamente:
- âœ… Baixar as imagens necessÃ¡rias
- âœ… Criar os containers
- âœ… Configurar volumes persistentes
- âœ… Subir MinIO (S3-compatible storage)
- âœ… Subir DuckDB (banco de dados analÃ­tico)
- âœ… Subir dbt (ferramenta de transformaÃ§Ã£o)
- âœ… Subir Jupyter Lab (ambiente de notebooks) - http://localhost:8888
- âœ… **Executar automaticamente todos os scripts Python:**
  - Criar bucket no MinIO
  - Gerar 5.000 registros de vendas fake
  - Criar tabela Iceberg `vendas_iceberg`
  - Inserir dados na tabela
  - Executar queries de exemplo
- âœ… **Executar automaticamente transformaÃ§Ãµes dbt:**
  - Modelos staging (limpeza de dados)
  - Modelos marts (modelos analÃ­ticos)
  - Testes de qualidade de dados

### 3. Processo AutomÃ¡tico de InicializaÃ§Ã£o

O sistema executa automaticamente na seguinte ordem:

#### Etapa 1: MinIO (Storage)
- MinIO sobe e fica disponÃ­vel nas portas 9000 (API) e 9001 (Console)
- Healthcheck garante que estÃ¡ pronto antes de continuar

#### Etapa 2: DuckDB (Banco de Dados)
- Container DuckDB sobe e fica aguardando conexÃµes
- Volume compartilhado `/app/lakehouse` Ã© criado para persistir dados

#### Etapa 3: InicializaÃ§Ã£o (init-service)
O serviÃ§o `init` executa automaticamente os seguintes scripts Python:

1. **`generate_fake_data.py`**
   - Gera 5.000 registros de vendas simulados
   - Salva em formato Parquet em `/app/data/vendas_raw.parquet`
   - Dados incluem: produtos, clientes, transaÃ§Ãµes, descontos, etc.

2. **`create_real_iceberg_table.py`**
   - Cria tabela **Iceberg REAL** com metadados completos
   - Gera estrutura Iceberg no MinIO (`s3://lakehouse/iceberg/vendas_real/`)
   - Cria tabela DuckDB `vendas_iceberg` para compatibilidade
   - Insere dados e cria snapshots iniciais

3. **`example_queries.py`**
   - Executa 8 queries analÃ­ticas de exemplo:
     - Receita por categoria
     - TendÃªncia de vendas mensal
     - Top clientes
     - AnÃ¡lise por canal
     - Time travel (demonstraÃ§Ã£o)
     - Schema evolution (demonstraÃ§Ã£o)
     - Performance de produtos
     - AnÃ¡lise regional

#### Etapa 4: TransformaÃ§Ãµes dbt (dbt-run-service)
ApÃ³s a inicializaÃ§Ã£o estar completa, o serviÃ§o `dbt-run` executa automaticamente:

1. **`dbt run`**
   - Executa todos os modelos dbt na ordem de dependÃªncia:
     - **Staging**: `stg_vendas` (limpeza e padronizaÃ§Ã£o)
     - **Marts**: 
       - `fct_vendas` (fato de vendas)
       - `dim_produtos` (dimensÃ£o de produtos)
       - `dim_clientes` (dimensÃ£o de clientes)
       - `mart_vendas_mensal` (agregaÃ§Ã£o mensal)

2. **`dbt test`**
   - Executa testes de qualidade de dados:
     - Verifica unicidade de chaves
     - Verifica valores nÃ£o nulos
     - Valida integridade referencial

### 4. Verificar Status

```bash
# Ver logs de todos os serviÃ§os
docker compose logs -f

# Ver logs de um serviÃ§o especÃ­fico
docker compose logs -f init        # InicializaÃ§Ã£o
docker compose logs -f dbt-run     # TransformaÃ§Ãµes dbt

# Verificar status dos containers
docker compose ps
```

### 5. Tempo Estimado

- **MinIO e DuckDB**: ~10-20 segundos
- **InicializaÃ§Ã£o (scripts Python)**: ~1-2 minutos
- **TransformaÃ§Ãµes dbt**: ~30 segundos
- **Total**: ~2-3 minutos

### 6. Verificar Sucesso

ApÃ³s a inicializaÃ§Ã£o, vocÃª deve ver mensagens como:

```
âœ“ INICIALIZAÃ‡ÃƒO CONCLUÃDA COM SUCESSO!
âœ“ dbt executado com sucesso!
```

Se tudo funcionou corretamente, vocÃª pode:
- Acessar MinIO Console: http://localhost:9001
- Acessar Jupyter Lab: http://localhost:8888
- Executar queries adicionais no DuckDB
- Explorar os modelos dbt criados
- Abrir notebooks para anÃ¡lise interativa

## ğŸ“ Estrutura do Projeto

```
modern-lakehouse-duckdb-iceberg/
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ duckdb/
â”‚   â”‚   â””â”€â”€ Dockerfile          # Imagem DuckDB + Python
â”‚   â”œâ”€â”€ dbt/
â”‚   â”‚   â””â”€â”€ Dockerfile          # Imagem dbt + DuckDB adapter
â”‚   â””â”€â”€ init/
â”‚       â””â”€â”€ Dockerfile          # Imagem de inicializaÃ§Ã£o
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/            # Modelos de staging (limpeza)
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_vendas.sql
â”‚   â”‚   â”‚   â””â”€â”€ schema.yml
â”‚   â”‚   â””â”€â”€ marts/              # Modelos analÃ­ticos
â”‚   â”‚       â”œâ”€â”€ fct_vendas.sql
â”‚   â”‚       â”œâ”€â”€ dim_produtos.sql
â”‚   â”‚       â”œâ”€â”€ dim_clientes.sql
â”‚   â”‚       â”œâ”€â”€ mart_vendas_mensal.sql
â”‚   â”‚       â””â”€â”€ schema.yml
â”‚   â”œâ”€â”€ dbt_project.yml         # ConfiguraÃ§Ã£o do projeto
â”‚   â””â”€â”€ profiles.yml            # Perfil de conexÃ£o
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_fake_data.py        # Gera dados fake
â”‚   â”œâ”€â”€ create_real_iceberg_table.py # Cria tabela Iceberg REAL
â”‚   â”œâ”€â”€ example_queries.py           # Queries de exemplo
â”‚   â””â”€â”€ init_lakehouse.py            # Script de inicializaÃ§Ã£o
â”œâ”€â”€ notebooks/                  # Notebooks Jupyter
â”‚   â”œâ”€â”€ test_duckdb_connection.ipynb  # Notebook de teste
â”‚   â””â”€â”€ README.md              # DocumentaÃ§Ã£o dos notebooks
â”œâ”€â”€ data/                       # Dados gerados (volumes)
â”œâ”€â”€ docker-compose.yml          # OrquestraÃ§Ã£o dos serviÃ§os
â””â”€â”€ README.md                   # Este arquivo
```

## âš™ï¸ Funcionalidades

### âœ… Funcionalidades Implementadas e Automatizadas

1. **CriaÃ§Ã£o AutomÃ¡tica de Bucket**
   - Bucket `lakehouse` criado automaticamente no MinIO durante inicializaÃ§Ã£o
   - ConfiguraÃ§Ã£o S3-compatible pronta para uso

2. **GeraÃ§Ã£o AutomÃ¡tica de Dados Fake**
   - 5.000 registros de vendas simulados gerados automaticamente
   - Dados realistas com produtos, clientes, descontos, canais de venda, etc
   - PerÃ­odo: 2023-2024 com distribuiÃ§Ã£o realista

3. **CriaÃ§Ã£o AutomÃ¡tica de Tabela Iceberg**
   - Tabela `vendas_iceberg` criada automaticamente no DuckDB
   - Dados inseridos automaticamente do arquivo Parquet gerado
   - Banco de dados persistente em volume compartilhado

4. **ExecuÃ§Ã£o AutomÃ¡tica de Queries AnalÃ­ticas**
   - 8 queries de exemplo executadas automaticamente durante inicializaÃ§Ã£o:
     - Receita por categoria
     - TendÃªncia de vendas mensal
     - Top clientes
     - AnÃ¡lise por canal
     - Time travel (demonstraÃ§Ã£o)
     - Schema evolution (demonstraÃ§Ã£o)
     - Performance de produtos
     - AnÃ¡lise regional

5. **TransformaÃ§Ãµes dbt AutomÃ¡ticas**
   - Modelos staging (limpeza) executados automaticamente
   - Modelos marts (anÃ¡lise) executados automaticamente
   - DimensÃµes e fatos criados automaticamente
   - Testes de qualidade executados automaticamente

## ğŸŒ Acessando os ServiÃ§os

### MinIO Console

**URL**: http://localhost:9001

**Credenciais**:
- UsuÃ¡rio: `admin`
- Senha: `minioadmin123`

No console vocÃª pode:
- Ver buckets e objetos
- Navegar pela estrutura de arquivos Iceberg
- Ver metadados

### DuckDB (via container)

```bash
# Entrar no container DuckDB
docker compose exec duckdb bash

# Executar Python interativo
python

# Ou executar scripts diretamente
docker compose exec duckdb python /app/scripts/example_queries.py
```

### dbt

```bash
# Entrar no container dbt
docker compose exec dbt bash

# Executar modelos
dbt run

# Executar testes
dbt test

# Gerar documentaÃ§Ã£o
dbt docs generate
dbt docs serve --port 8080
```

## ğŸ““ Notebooks Jupyter

O projeto inclui um ambiente **Jupyter Lab** totalmente integrado com o ecossistema do Lakehouse, permitindo anÃ¡lise interativa e exploraÃ§Ã£o de dados.

### Acessar Jupyter Lab

**URL**: http://localhost:8888

O Jupyter Lab inicia automaticamente quando vocÃª executa `docker compose up`. NÃ£o requer autenticaÃ§Ã£o (apenas para desenvolvimento local).

### Funcionalidades

- âœ… **ConexÃ£o direta ao DuckDB**: Acesse o banco de dados compartilhado
- âœ… **IntegraÃ§Ã£o com MinIO**: Configure e acesse dados no MinIO via S3
- âœ… **Acesso Ã s tabelas do dbt**: Consulte modelos transformados (dim_*, fct_*, mart_*)
- âœ… **VisualizaÃ§Ãµes**: Bibliotecas matplotlib, seaborn e plotly incluÃ­das
- âœ… **Scripts disponÃ­veis**: Acesso aos scripts Python do projeto

### Notebooks DisponÃ­veis

- **`test_duckdb_connection.ipynb`**: Notebook de teste que demonstra:
  - ConexÃ£o com DuckDB e MinIO
  - ExecuÃ§Ã£o de queries baseadas em `scripts/example_queries.py`
  - VisualizaÃ§Ãµes de dados
  - VerificaÃ§Ã£o de tabelas do dbt

### Iniciar o ServiÃ§o

```bash
# Iniciar apenas o Jupyter (e dependÃªncias)
docker compose up -d jupyter

# Ou iniciar tudo
docker compose up -d
```

### Estrutura de Volumes

Os notebooks tÃªm acesso a:
- `./notebooks` â†’ `/app/notebooks` - Seus notebooks
- `./scripts` â†’ `/app/scripts` - Scripts Python do projeto
- `./data` â†’ `/app/data` - Dados brutos
- `./dbt` â†’ `/app/dbt` - Projeto dbt
- `lakehouse_data` â†’ `/app/lakehouse` - Banco DuckDB compartilhado

### Exemplo RÃ¡pido

```python
import duckdb
import os

# Conectar ao DuckDB compartilhado
con = duckdb.connect("/app/lakehouse/lakehouse.duckdb")

# Executar query
df = con.execute("SELECT * FROM vendas_iceberg LIMIT 10").fetchdf()
print(df)
```

ğŸ“– **Para mais detalhes, consulte o [README dos notebooks](notebooks/README.md)**

## ğŸ’¡ Exemplos de Uso

### âš¡ Tudo Ã© AutomÃ¡tico!

**Importante**: Todos os scripts Python e transformaÃ§Ãµes dbt sÃ£o executados automaticamente quando vocÃª roda `docker compose up`. VocÃª nÃ£o precisa executar nada manualmente!

### 1. Re-executar Queries de Exemplo (Opcional)

Se quiser executar as queries novamente:

```bash
docker compose exec duckdb python /app/scripts/example_queries.py
```

Isso executarÃ¡ 8 queries demonstrando:
- Receita por categoria
- TendÃªncia de vendas mensal
- Top clientes
- AnÃ¡lise por canal
- Time travel
- Schema evolution
- Performance de produtos
- AnÃ¡lise regional

### 2. Re-executar TransformaÃ§Ãµes dbt (Opcional)

Se quiser executar as transformaÃ§Ãµes dbt novamente:

```bash
# Executar todos os modelos
docker compose exec dbt dbt run

# Executar apenas staging
docker compose exec dbt dbt run --select staging

# Executar apenas marts
docker compose exec dbt dbt run --select marts

# Executar testes
docker compose exec dbt dbt test
```

### 3. Query Direta no DuckDB

```bash
docker compose exec duckdb python
```

```python
import duckdb
import os

# Conectar
con = duckdb.connect()

# Configurar S3
con.execute("""
    INSTALL httpfs;
    LOAD httpfs;
    SET s3_endpoint='minio:9000';
    SET s3_access_key_id='admin';
    SET s3_secret_access_key='minioadmin123';
    SET s3_use_ssl=false;
    SET s3_url_style='path';
""")

# Query
result = con.execute("""
    SELECT 
        categoria,
        COUNT(*) as total,
        SUM(valor_final) as receita
    FROM vendas_iceberg
    GROUP BY categoria
    ORDER BY receita DESC;
""").fetchdf()

print(result)
```

### 4. Adicionar Mais Dados (Opcional)

Se quiser gerar mais dados alÃ©m dos 5.000 iniciais:

```bash
# Gerar mais dados (edite o script para alterar quantidade)
docker compose exec duckdb python /app/scripts/generate_fake_data.py

# Inserir na tabela Iceberg
docker compose exec duckdb python -c "
import duckdb
con = duckdb.connect('/app/lakehouse/lakehouse.duckdb')
con.execute('INSTALL httpfs; LOAD httpfs;')
con.execute('INSTALL iceberg; LOAD iceberg;')
con.execute(\"SET s3_endpoint='minio:9000';\")
con.execute(\"SET s3_access_key_id='admin';\")
con.execute(\"SET s3_secret_access_key='minioadmin123';\")
con.execute(\"SET s3_use_ssl=false;\")
con.execute(\"SET s3_url_style='path';\")
con.execute(\"INSERT INTO vendas_iceberg SELECT * FROM read_parquet('/app/data/vendas_raw.parquet');\")
"
```

### 5. Explorar Metadados Iceberg

```bash
# Listar snapshots (via MinIO Console ou cÃ³digo)
docker compose exec duckdb python -c "
import duckdb
con = duckdb.connect()
# Configurar S3...
# Consultar metadados Iceberg
"
```

## ğŸ”„ ComparaÃ§Ã£o com Databricks

Este projeto simula uma arquitetura similar ao **Databricks Lakehouse**:

| Recurso | Databricks | Este Projeto |
|---------|-----------|--------------|
| **Storage** | DBFS / S3 / ADLS | MinIO (S3-compatible) |
| **Table Format** | Delta Lake | Apache Iceberg |
| **Query Engine** | Spark SQL | DuckDB |
| **Transform** | dbt / Spark | dbt |
| **Time Travel** | âœ… Sim | âœ… Sim (Iceberg) |
| **Schema Evolution** | âœ… Sim | âœ… Sim (Iceberg) |
| **ACID** | âœ… Sim | âœ… Sim (Iceberg) |
| **UI** | Databricks Notebooks | Jupyter Lab / Docker CLI / MinIO Console |

### Vantagens deste Projeto (100% Open Source)

- âœ… **100% Local**: Roda completamente offline
- âœ… **Zero Custo**: Sem necessidade de cloud ou licenÃ§as
- âœ… **100% Open Source**: Todas as tecnologias sÃ£o open source (DuckDB, Iceberg, MinIO, dbt, Jupyter)
- âœ… **Educacional**: Ideal para aprender conceitos de Lakehouse
- âœ… **RÃ¡pido Setup**: `docker compose up` e pronto
- âœ… **Alternativa Open Source**: Substitui soluÃ§Ãµes proprietÃ¡rias como Databricks

### LimitaÃ§Ãµes vs Databricks

- âš ï¸ **Escala**: Limitado a mÃ¡quina local (vs cluster distribuÃ­do)
- âš ï¸ **ColaboraÃ§Ã£o**: Notebooks locais (vs notebooks compartilhados na nuvem)
- âš ï¸ **ML**: Sem MLflow integrado
- âš ï¸ **GovernanÃ§a**: Sem Unity Catalog
- âš ï¸ **Performance**: DuckDB Ã© single-node (vs Spark distribuÃ­do)

## ğŸ› ï¸ Troubleshooting

### Problema: MinIO nÃ£o inicia

**Sintomas**: Container minio para ou nÃ£o responde

**SoluÃ§Ã£o**:
```bash
# Verificar logs
docker compose logs minio

# Verificar se a porta estÃ¡ em uso
netstat -an | grep 9000

# Reiniciar serviÃ§o
docker compose restart minio

# Se persistir, recriar volumes
docker compose down -v
docker compose up
```

### Problema: Tabela Iceberg nÃ£o encontrada

**Sintomas**: Erro "Table 'vendas_iceberg' not found" ao executar queries

**SoluÃ§Ã£o**:
```bash
# Verificar se a inicializaÃ§Ã£o foi concluÃ­da
docker compose logs init

# Re-executar inicializaÃ§Ã£o completa
docker compose down
docker compose up init

# Verificar se o banco foi criado
docker compose exec duckdb ls -lh /app/lakehouse/
```

### Problema: Erro de conexÃ£o S3

**Sintomas**: Erro ao conectar ao MinIO (timeout, connection refused)

**SoluÃ§Ã£o**:
```bash
# Verificar se MinIO estÃ¡ rodando
docker compose ps minio

# Verificar healthcheck
docker compose exec minio curl -f http://localhost:9000/minio/health/live

# Verificar variÃ¡veis de ambiente
docker compose config | grep MINIO
```

### Problema: dbt nÃ£o encontra tabela

**Sintomas**: Erro "Table 'vendas_iceberg' does not exist" no dbt

**SoluÃ§Ã£o**:
```bash
# Verificar logs da inicializaÃ§Ã£o
docker compose logs init

# Verificar se o banco existe
docker compose exec dbt-run ls -lh /app/lakehouse/

# Re-executar inicializaÃ§Ã£o e dbt
docker compose up init
docker compose up dbt-run
```

### Problema: Scripts Python falham

**Sintomas**: Erros ao executar scripts de inicializaÃ§Ã£o

**SoluÃ§Ã£o**:
```bash
# Verificar logs detalhados
docker compose logs init

# Executar script manualmente para debug
docker compose exec duckdb python /app/scripts/generate_fake_data.py
docker compose exec duckdb python /app/scripts/create_real_iceberg_table.py

# Verificar dependÃªncias
docker compose exec duckdb pip list
```

### Problema: dbt run falha

**Sintomas**: Erro ao executar `dbt run`

**SoluÃ§Ã£o**:
```bash
# Verificar logs
docker compose logs dbt-run

# Executar dbt manualmente para debug
docker compose exec dbt dbt debug
docker compose exec dbt dbt run --select staging
docker compose exec dbt dbt run --select marts

# Verificar se o banco estÃ¡ acessÃ­vel
docker compose exec dbt python -c "import duckdb; con = duckdb.connect('/app/lakehouse/lakehouse.duckdb'); print(con.execute('SELECT COUNT(*) FROM vendas_iceberg').fetchone())"
```

### Problema: Containers param imediatamente

**Sintomas**: Containers iniciam e param logo em seguida

**SoluÃ§Ã£o**:
```bash
# Verificar logs de todos os serviÃ§os
docker compose logs

# Verificar status
docker compose ps -a

# Recriar tudo do zero
docker compose down -v
docker compose build --no-cache
docker compose up
```

### Limpar tudo e comeÃ§ar do zero

Se nada funcionar, limpe tudo e recomece:

```bash
# Parar e remover tudo
docker compose down -v

# Remover imagens (opcional)
docker compose down --rmi all

# Reconstruir e iniciar
docker compose build --no-cache
docker compose up
```

## ğŸ“ PrÃ³ximos Passos

Ideias para expandir o projeto:

- [ ] Adicionar mais tabelas (clientes, produtos separados)
- [ ] Implementar streaming de dados
- [ ] Adicionar testes automatizados
- [ ] Criar dashboards (Grafana/Metabase)
- [ ] Implementar CI/CD
- [ ] Adicionar Airflow para orquestraÃ§Ã£o
- [ ] Implementar data quality checks

## ğŸ“„ LicenÃ§a

Este projeto Ã© open source e estÃ¡ disponÃ­vel para fins educacionais.

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para abrir issues ou pull requests.

## ğŸ“š ReferÃªncias

### DocumentaÃ§Ã£o Externa

- [Apache Iceberg](https://iceberg.apache.org/)
- [DuckDB](https://duckdb.org/)
- [MinIO](https://min.io/)
- [dbt](https://www.getdbt.com/)
- [Databricks Lakehouse](https://www.databricks.com/product/data-lakehouse)

### DocumentaÃ§Ã£o do Projeto

- [ImplementaÃ§Ã£o Apache Iceberg](scripts/ICEBERG_IMPLEMENTATION.md) - Detalhes tÃ©cnicos sobre a implementaÃ§Ã£o do Iceberg neste projeto

---

**Desenvolvido com â¤ï¸ para aprendizado de engenharia de dados modernos**
