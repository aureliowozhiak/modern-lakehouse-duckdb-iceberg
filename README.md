# Modern Opensource Lakehouse: DuckDB + Apache Iceberg + MinIO + dbt

Laborat√≥rio pr√°tico de um **Lakehouse moderno e 100% open source** totalmente containerizado, demonstrando conceitos avan√ßados de engenharia de dados como versionamento, time travel, schema evolution e transforma√ß√µes com dbt.


> üöÄ **Reproduza um Lakehouse moderno, feature-complete e open source em minutos usando DuckDB, Iceberg, MinIO e dbt ‚Äì tudo orquestrado via Docker Compose.**
>
> - **Time travel**, **schema evolution** e **tables versionadas**
> - Transforma dados usando **dbt** (Python)
> - Exemplo 100% pr√°tico + scripts did√°ticos
> - **Notebooks Jupyter** prontos para an√°lise
> - Deploy local, 100% open (sem cloud/lock-in!)

[![Reposit√≥rio no GitHub](https://img.shields.io/github/stars/aureliowozhiak/modern-lakehouse-duckdb-iceberg?style=social)](https://github.com/aureliowozhiak/modern-lakehouse-duckdb-iceberg)

---

- **Reposit√≥rio:** [https://github.com/aureliowozhiak/modern-lakehouse-duckdb-iceberg](https://github.com/aureliowozhiak/modern-lakehouse-duckdb-iceberg)
- **Documenta√ß√£o completa:** Veja exemplos de queries, screenshots, v√≠deos e detalhes t√©cnicos no [reposit√≥rio do GitHub](https://github.com/aureliowozhiak/modern-lakehouse-duckdb-iceberg)



## üìã √çndice

- [Vis√£o Geral](#vis√£o-geral)
- [Arquitetura](#arquitetura)
- [Conceitos](#conceitos)
- [Pr√©-requisitos](#pr√©-requisitos)
- [Instala√ß√£o e Uso](#instala√ß√£o-e-uso)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [Funcionalidades](#funcionalidades)
- [Acessando os Servi√ßos](#acessando-os-servi√ßos)
- [Notebooks Jupyter](#notebooks-jupyter)
- [Exemplos de Uso](#exemplos-de-uso)
- [Compara√ß√£o com Databricks](#compara√ß√£o-com-databricks)

## üéØ Vis√£o Geral

Este projeto implementa um **Lakehouse 100% open source** completo em ambiente local usando:

- **DuckDB**: Engine anal√≠tico in-memory otimizado para OLAP
- **Apache Iceberg**: Tabela format para versionamento e time travel
- **MinIO**: Storage S3-compatible para simular cloud storage
- **dbt**: Ferramenta de transforma√ß√£o de dados (ELT)
- **Jupyter Lab**: Ambiente de notebooks integrado para an√°lise e explora√ß√£o
- **Docker Compose**: Orquestra√ß√£o de todos os servi√ßos

### O que √© um Lakehouse?

Um **Lakehouse** combina as melhores caracter√≠sticas de um **Data Lake** (armazenamento barato, formatos abertos) com as de um **Data Warehouse** (ACID transactions, schema enforcement, performance).

**Vantagens:**
- ‚úÖ Armazenamento econ√¥mico (formato Parquet/Delta/Iceberg)
- ‚úÖ Suporte a dados estruturados, semi-estruturados e n√£o estruturados
- ‚úÖ ACID transactions e versionamento
- ‚úÖ Time travel (acessar vers√µes anteriores dos dados)
- ‚úÖ Schema evolution (evoluir schema sem quebrar compatibilidade)
- ‚úÖ Performance de queries anal√≠ticas
- ‚úÖ Integra√ß√£o com ferramentas modernas (Spark, dbt, etc)

## üèóÔ∏è Arquitetura

<p align="center">
  <img src="https://raw.githubusercontent.com/aureliowozhiak/modern-lakehouse-duckdb-iceberg/main/docs/lakehouse-diagram.png" alt="Arquitetura do Lakehouse" width="800"/>
</p>

### Fluxo de Dados

1. **Inicializa√ß√£o**: O servi√ßo `init` cria o bucket no MinIO, gera dados fake e cria a tabela Iceberg
2. **Armazenamento**: Dados s√£o armazenados no MinIO (S3-compatible) em formato Iceberg
3. **An√°lise**: DuckDB consulta diretamente as tabelas Iceberg no MinIO
4. **Transforma√ß√£o**: dbt transforma os dados brutos em modelos anal√≠ticos (marts)

## üìö Conceitos

### Apache Iceberg

**Apache Iceberg** √© uma especifica√ß√£o de tabela aberta para analytics em data lakes. Ele fornece:

- **ACID Transactions**: Garante consist√™ncia dos dados
- **Time Travel**: Acesse vers√µes anteriores dos dados
- **Schema Evolution**: Adicione/remova colunas sem quebrar queries antigas
- **Hidden Partitioning**: Particionamento autom√°tico e otimizado
- **Metadata Management**: Metadados versionados e eficientes

#### Implementa√ß√£o no Projeto

Este projeto implementa **Iceberg real** usando:

1. **Tabela DuckDB** (`vendas_iceberg`): Tabela persistente no DuckDB para uso imediato
2. **Tabela Iceberg Real** (`s3://lakehouse/iceberg/vendas_real/`): Estrutura Iceberg completa com:
   - Metadados versionados (`metadata/*.metadata.json`)
   - Arquivos de dados Parquet (`data/*.parquet`)
   - Snapshots para time travel
   - Estrutura de diret√≥rios compat√≠vel com Iceberg

**Script dispon√≠vel:**
- `create_real_iceberg_table.py`: Cria tabela Iceberg REAL com metadados completos

üìñ **Para mais detalhes t√©cnicos sobre a implementa√ß√£o do Iceberg, consulte:** [`scripts/ICEBERG_IMPLEMENTATION.md`](scripts/ICEBERG_IMPLEMENTATION.md)

**Exemplo de Time Travel:**
```sql
-- Ver dados de 1 hora atr√°s
SELECT * FROM vendas_iceberg 
FOR TIMESTAMP AS OF '2024-01-01 10:00:00';

-- Ver snapshot espec√≠fico
SELECT * FROM vendas_iceberg 
FOR VERSION AS OF 5;
```

**Exemplo de Schema Evolution:**
```sql
-- Adicionar nova coluna sem quebrar queries antigas
ALTER TABLE vendas_iceberg 
ADD COLUMN novo_campo VARCHAR;
```

### DuckDB

**DuckDB** √© um banco de dados anal√≠tico in-memory otimizado para OLAP (Online Analytical Processing). Caracter√≠sticas:

- ‚ö° Performance excepcional para queries anal√≠ticas
- üîå Integra√ß√£o nativa com Parquet, CSV, JSON
- üì¶ Extens√µes para S3, Iceberg, Postgres, etc
- üêç Integra√ß√£o Python/R f√°cil
- üíæ Zero configura√ß√£o

### MinIO

**MinIO** √© um servidor de armazenamento de objetos S3-compatible. Usado aqui para simular cloud storage localmente.

## üöÄ Pr√©-requisitos

- **Docker** (vers√£o 20.10+)
- **Docker Compose** (vers√£o 2.0+)
- **Git** (para clonar o reposit√≥rio)

## üì¶ Instala√ß√£o e Uso

### 1. Clone o reposit√≥rio

```bash
git clone <repo-url>
cd modern-lakehouse-duckdb-iceberg
```

### 2. Inicie os servi√ßos

```bash
docker compose up
```

**‚ö†Ô∏è Importante**: Execute sem `-d` na primeira vez para ver os logs em tempo real e garantir que tudo est√° funcionando.

Este comando ir√° automaticamente:
- ‚úÖ Baixar as imagens necess√°rias
- ‚úÖ Criar os containers
- ‚úÖ Configurar volumes persistentes
- ‚úÖ Subir MinIO (S3-compatible storage)
- ‚úÖ Subir DuckDB (banco de dados anal√≠tico)
- ‚úÖ Subir dbt (ferramenta de transforma√ß√£o)
- ‚úÖ Subir Jupyter Lab (ambiente de notebooks) - http://localhost:8888
- ‚úÖ **Executar automaticamente todos os scripts Python:**
  - Criar bucket no MinIO
  - Gerar 5.000 registros de vendas fake
  - Criar tabela Iceberg `vendas_iceberg`
  - Inserir dados na tabela
  - Executar queries de exemplo
- ‚úÖ **Executar automaticamente transforma√ß√µes dbt:**
  - Modelos staging (limpeza de dados)
  - Modelos marts (modelos anal√≠ticos)
  - Testes de qualidade de dados

### 3. Processo Autom√°tico de Inicializa√ß√£o

O sistema executa automaticamente na seguinte ordem:

#### Etapa 1: MinIO (Storage)
- MinIO sobe e fica dispon√≠vel nas portas 9000 (API) e 9001 (Console)
- Healthcheck garante que est√° pronto antes de continuar

#### Etapa 2: DuckDB (Banco de Dados)
- Container DuckDB sobe e fica aguardando conex√µes
- Volume compartilhado `/app/lakehouse` √© criado para persistir dados

#### Etapa 3: Inicializa√ß√£o (init-service)
O servi√ßo `init` executa automaticamente os seguintes scripts Python:

1. **`generate_fake_data.py`**
   - Gera 5.000 registros de vendas simulados
   - Salva em formato Parquet em `/app/data/vendas_raw.parquet`
   - Dados incluem: produtos, clientes, transa√ß√µes, descontos, etc.

2. **`create_real_iceberg_table.py`**
   - Cria tabela **Iceberg REAL** com metadados completos
   - Gera estrutura Iceberg no MinIO (`s3://lakehouse/iceberg/vendas_real/`)
   - Cria tabela DuckDB `vendas_iceberg` para compatibilidade
   - Insere dados e cria snapshots iniciais

3. **`example_queries.py`**
   - Executa 8 queries anal√≠ticas de exemplo:
     - Receita por categoria
     - Tend√™ncia de vendas mensal
     - Top clientes
     - An√°lise por canal
     - Time travel (demonstra√ß√£o)
     - Schema evolution (demonstra√ß√£o)
     - Performance de produtos
     - An√°lise regional

#### Etapa 4: Transforma√ß√µes dbt (dbt-run-service)
Ap√≥s a inicializa√ß√£o estar completa, o servi√ßo `dbt-run` executa automaticamente:

1. **`dbt run`**
   - Executa todos os modelos dbt na ordem de depend√™ncia:
     - **Staging**: `stg_vendas` (limpeza e padroniza√ß√£o)
     - **Marts**: 
       - `fct_vendas` (fato de vendas)
       - `dim_produtos` (dimens√£o de produtos)
       - `dim_clientes` (dimens√£o de clientes)
       - `mart_vendas_mensal` (agrega√ß√£o mensal)

2. **`dbt test`**
   - Executa testes de qualidade de dados:
     - Verifica unicidade de chaves
     - Verifica valores n√£o nulos
     - Valida integridade referencial

### 4. Verificar Status

```bash
# Ver logs de todos os servi√ßos
docker compose logs -f

# Ver logs de um servi√ßo espec√≠fico
docker compose logs -f init        # Inicializa√ß√£o
docker compose logs -f dbt-run     # Transforma√ß√µes dbt

# Verificar status dos containers
docker compose ps
```

### 5. Tempo Estimado

- **MinIO e DuckDB**: ~10-20 segundos
- **Inicializa√ß√£o (scripts Python)**: ~1-2 minutos
- **Transforma√ß√µes dbt**: ~30 segundos
- **Total**: ~2-3 minutos

### 6. Verificar Sucesso

Ap√≥s a inicializa√ß√£o, voc√™ deve ver mensagens como:

```
‚úì INICIALIZA√á√ÉO CONCLU√çDA COM SUCESSO!
‚úì dbt executado com sucesso!
```

Se tudo funcionou corretamente, voc√™ pode:
- Acessar MinIO Console: http://localhost:9001
- Acessar Jupyter Lab: http://localhost:8888
- Executar queries adicionais no DuckDB
- Explorar os modelos dbt criados
- Abrir notebooks para an√°lise interativa

## üìÅ Estrutura do Projeto

```
modern-lakehouse-duckdb-iceberg/
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ duckdb/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile          # Imagem DuckDB + Python
‚îÇ   ‚îú‚îÄ‚îÄ dbt/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile          # Imagem dbt + DuckDB adapter
‚îÇ   ‚îî‚îÄ‚îÄ init/
‚îÇ       ‚îî‚îÄ‚îÄ Dockerfile          # Imagem de inicializa√ß√£o
‚îú‚îÄ‚îÄ dbt/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ staging/            # Modelos de staging (limpeza)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stg_vendas.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schema.yml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ marts/              # Modelos anal√≠ticos
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ fct_vendas.sql
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ dim_produtos.sql
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ dim_clientes.sql
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ mart_vendas_mensal.sql
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ schema.yml
‚îÇ   ‚îú‚îÄ‚îÄ dbt_project.yml         # Configura√ß√£o do projeto
‚îÇ   ‚îî‚îÄ‚îÄ profiles.yml            # Perfil de conex√£o
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ generate_fake_data.py        # Gera dados fake
‚îÇ   ‚îú‚îÄ‚îÄ create_real_iceberg_table.py # Cria tabela Iceberg REAL
‚îÇ   ‚îú‚îÄ‚îÄ example_queries.py           # Queries de exemplo
‚îÇ   ‚îî‚îÄ‚îÄ init_lakehouse.py            # Script de inicializa√ß√£o
‚îú‚îÄ‚îÄ notebooks/                  # Notebooks Jupyter
‚îÇ   ‚îú‚îÄ‚îÄ test_duckdb_connection.ipynb  # Notebook de teste
‚îÇ   ‚îî‚îÄ‚îÄ README.md              # Documenta√ß√£o dos notebooks
‚îú‚îÄ‚îÄ data/                       # Dados gerados (volumes)
‚îú‚îÄ‚îÄ docker-compose.yml          # Orquestra√ß√£o dos servi√ßos
‚îî‚îÄ‚îÄ README.md                   # Este arquivo
```

## ‚öôÔ∏è Funcionalidades

### ‚úÖ Funcionalidades Implementadas e Automatizadas

1. **Cria√ß√£o Autom√°tica de Bucket**
   - Bucket `lakehouse` criado automaticamente no MinIO durante inicializa√ß√£o
   - Configura√ß√£o S3-compatible pronta para uso

2. **Gera√ß√£o Autom√°tica de Dados Fake**
   - 5.000 registros de vendas simulados gerados automaticamente
   - Dados realistas com produtos, clientes, descontos, canais de venda, etc
   - Per√≠odo: 2023-2024 com distribui√ß√£o realista

3. **Cria√ß√£o Autom√°tica de Tabela Iceberg**
   - Tabela `vendas_iceberg` criada automaticamente no DuckDB
   - Dados inseridos automaticamente do arquivo Parquet gerado
   - Banco de dados persistente em volume compartilhado

4. **Execu√ß√£o Autom√°tica de Queries Anal√≠ticas**
   - 8 queries de exemplo executadas automaticamente durante inicializa√ß√£o:
     - Receita por categoria
     - Tend√™ncia de vendas mensal
     - Top clientes
     - An√°lise por canal
     - Time travel (demonstra√ß√£o)
     - Schema evolution (demonstra√ß√£o)
     - Performance de produtos
     - An√°lise regional

5. **Transforma√ß√µes dbt Autom√°ticas**
   - Modelos staging (limpeza) executados automaticamente
   - Modelos marts (an√°lise) executados automaticamente
   - Dimens√µes e fatos criados automaticamente
   - Testes de qualidade executados automaticamente

## üåê Acessando os Servi√ßos

### MinIO Console

**URL**: http://localhost:9001

**Credenciais**:
- Usu√°rio: `admin`
- Senha: `minioadmin123`

No console voc√™ pode:
- Ver buckets e objetos
- Navegar pela estrutura de arquivos Iceberg
- Ver metadados

### DuckDB (via container)

```bash
# Entrar no container DuckDB
docker compose exec duckdb bash

# Executar Python interativo
python

# Ou executar scripts diretamente
docker compose exec duckdb python /app/scripts/example_queries.py
```

### dbt

```bash
# Entrar no container dbt
docker compose exec dbt bash

# Executar modelos
dbt run

# Executar testes
dbt test

# Gerar documenta√ß√£o
dbt docs generate
dbt docs serve --port 8080
```

## üìì Notebooks Jupyter

O projeto inclui um ambiente **Jupyter Lab** totalmente integrado com o ecossistema do Lakehouse, permitindo an√°lise interativa e explora√ß√£o de dados.

### Acessar Jupyter Lab

**URL**: http://localhost:8888

O Jupyter Lab inicia automaticamente quando voc√™ executa `docker compose up`. N√£o requer autentica√ß√£o (apenas para desenvolvimento local).

### Funcionalidades

- ‚úÖ **Conex√£o direta ao DuckDB**: Acesse o banco de dados compartilhado
- ‚úÖ **Integra√ß√£o com MinIO**: Configure e acesse dados no MinIO via S3
- ‚úÖ **Acesso √†s tabelas do dbt**: Consulte modelos transformados (dim_*, fct_*, mart_*)
- ‚úÖ **Visualiza√ß√µes**: Bibliotecas matplotlib, seaborn e plotly inclu√≠das
- ‚úÖ **Scripts dispon√≠veis**: Acesso aos scripts Python do projeto

### Notebooks Dispon√≠veis

- **`test_duckdb_connection.ipynb`**: Notebook de teste que demonstra:
  - Conex√£o com DuckDB e MinIO
  - Execu√ß√£o de queries baseadas em `scripts/example_queries.py`
  - Visualiza√ß√µes de dados
  - Verifica√ß√£o de tabelas do dbt

### Iniciar o Servi√ßo

```bash
# Iniciar apenas o Jupyter (e depend√™ncias)
docker compose up -d jupyter

# Ou iniciar tudo
docker compose up -d
```

### Estrutura de Volumes

Os notebooks t√™m acesso a:
- `./notebooks` ‚Üí `/app/notebooks` - Seus notebooks
- `./scripts` ‚Üí `/app/scripts` - Scripts Python do projeto
- `./data` ‚Üí `/app/data` - Dados brutos
- `./dbt` ‚Üí `/app/dbt` - Projeto dbt
- `lakehouse_data` ‚Üí `/app/lakehouse` - Banco DuckDB compartilhado

### Exemplo R√°pido

```python
import duckdb
import os

# Conectar ao DuckDB compartilhado
con = duckdb.connect("/app/lakehouse/lakehouse.duckdb")

# Executar query
df = con.execute("SELECT * FROM vendas_iceberg LIMIT 10").fetchdf()
print(df)
```

üìñ **Para mais detalhes, consulte o [README dos notebooks](notebooks/README.md)**

## üí° Exemplos de Uso

### ‚ö° Tudo √© Autom√°tico!

**Importante**: Todos os scripts Python e transforma√ß√µes dbt s√£o executados automaticamente quando voc√™ roda `docker compose up`. Voc√™ n√£o precisa executar nada manualmente!

### 1. Re-executar Queries de Exemplo (Opcional)

Se quiser executar as queries novamente:

```bash
docker compose exec duckdb python /app/scripts/example_queries.py
```

Isso executar√° 8 queries demonstrando:
- Receita por categoria
- Tend√™ncia de vendas mensal
- Top clientes
- An√°lise por canal
- Time travel
- Schema evolution
- Performance de produtos
- An√°lise regional

### 2. Re-executar Transforma√ß√µes dbt (Opcional)

Se quiser executar as transforma√ß√µes dbt novamente:

```bash
# Executar todos os modelos
docker compose exec dbt dbt run

# Executar apenas staging
docker compose exec dbt dbt run --select staging

# Executar apenas marts
docker compose exec dbt dbt run --select marts

# Executar testes
docker compose exec dbt dbt test
```

### 3. Query Direta no DuckDB

```bash
docker compose exec duckdb python
```

```python
import duckdb
import os

# Conectar
con = duckdb.connect()

# Configurar S3
con.execute("""
    INSTALL httpfs;
    LOAD httpfs;
    SET s3_endpoint='minio:9000';
    SET s3_access_key_id='admin';
    SET s3_secret_access_key='minioadmin123';
    SET s3_use_ssl=false;
    SET s3_url_style='path';
""")

# Query
result = con.execute("""
    SELECT 
        categoria,
        COUNT(*) as total,
        SUM(valor_final) as receita
    FROM vendas_iceberg
    GROUP BY categoria
    ORDER BY receita DESC;
""").fetchdf()

print(result)
```

### 4. Adicionar Mais Dados (Opcional)

Se quiser gerar mais dados al√©m dos 5.000 iniciais:

```bash
# Gerar mais dados (edite o script para alterar quantidade)
docker compose exec duckdb python /app/scripts/generate_fake_data.py

# Inserir na tabela Iceberg
docker compose exec duckdb python -c "
import duckdb
con = duckdb.connect('/app/lakehouse/lakehouse.duckdb')
con.execute('INSTALL httpfs; LOAD httpfs;')
con.execute('INSTALL iceberg; LOAD iceberg;')
con.execute(\"SET s3_endpoint='minio:9000';\")
con.execute(\"SET s3_access_key_id='admin';\")
con.execute(\"SET s3_secret_access_key='minioadmin123';\")
con.execute(\"SET s3_use_ssl=false;\")
con.execute(\"SET s3_url_style='path';\")
con.execute(\"INSERT INTO vendas_iceberg SELECT * FROM read_parquet('/app/data/vendas_raw.parquet');\")
"
```

### 5. Explorar Metadados Iceberg

```bash
# Listar snapshots (via MinIO Console ou c√≥digo)
docker compose exec duckdb python -c "
import duckdb
con = duckdb.connect()
# Configurar S3...
# Consultar metadados Iceberg
"
```

## üîÑ Compara√ß√£o com Databricks

Este projeto simula uma arquitetura similar ao **Databricks Lakehouse**:

| Recurso | Databricks | Este Projeto |
|---------|-----------|--------------|
| **Storage** | DBFS / S3 / ADLS | MinIO (S3-compatible) |
| **Table Format** | Delta Lake | Apache Iceberg |
| **Query Engine** | Spark SQL | DuckDB |
| **Transform** | dbt / Spark | dbt |
| **Time Travel** | ‚úÖ Sim | ‚úÖ Sim (Iceberg) |
| **Schema Evolution** | ‚úÖ Sim | ‚úÖ Sim (Iceberg) |
| **ACID** | ‚úÖ Sim | ‚úÖ Sim (Iceberg) |
| **UI** | Databricks Notebooks | Jupyter Lab / Docker CLI / MinIO Console |

### Vantagens deste Projeto (100% Open Source)

- ‚úÖ **100% Local**: Roda completamente offline
- ‚úÖ **Zero Custo**: Sem necessidade de cloud ou licen√ßas
- ‚úÖ **100% Open Source**: Todas as tecnologias s√£o open source (DuckDB, Iceberg, MinIO, dbt, Jupyter)
- ‚úÖ **Educacional**: Ideal para aprender conceitos de Lakehouse
- ‚úÖ **R√°pido Setup**: `docker compose up` e pronto
- ‚úÖ **Alternativa Open Source**: Substitui solu√ß√µes propriet√°rias como Databricks

### Limita√ß√µes vs Databricks

- ‚ö†Ô∏è **Escala**: Limitado a m√°quina local (vs cluster distribu√≠do)
- ‚ö†Ô∏è **Colabora√ß√£o**: Notebooks locais (vs notebooks compartilhados na nuvem)
- ‚ö†Ô∏è **ML**: Sem MLflow integrado
- ‚ö†Ô∏è **Governan√ßa**: Sem Unity Catalog
- ‚ö†Ô∏è **Performance**: DuckDB √© single-node (vs Spark distribu√≠do)

## üõ†Ô∏è Troubleshooting

### Problema: MinIO n√£o inicia

**Sintomas**: Container minio para ou n√£o responde

**Solu√ß√£o**:
```bash
# Verificar logs
docker compose logs minio

# Verificar se a porta est√° em uso
netstat -an | grep 9000

# Reiniciar servi√ßo
docker compose restart minio

# Se persistir, recriar volumes
docker compose down -v
docker compose up
```

### Problema: Tabela Iceberg n√£o encontrada

**Sintomas**: Erro "Table 'vendas_iceberg' not found" ao executar queries

**Solu√ß√£o**:
```bash
# Verificar se a inicializa√ß√£o foi conclu√≠da
docker compose logs init

# Re-executar inicializa√ß√£o completa
docker compose down
docker compose up init

# Verificar se o banco foi criado
docker compose exec duckdb ls -lh /app/lakehouse/
```

### Problema: Erro de conex√£o S3

**Sintomas**: Erro ao conectar ao MinIO (timeout, connection refused)

**Solu√ß√£o**:
```bash
# Verificar se MinIO est√° rodando
docker compose ps minio

# Verificar healthcheck
docker compose exec minio curl -f http://localhost:9000/minio/health/live

# Verificar vari√°veis de ambiente
docker compose config | grep MINIO
```

### Problema: dbt n√£o encontra tabela

**Sintomas**: Erro "Table 'vendas_iceberg' does not exist" no dbt

**Solu√ß√£o**:
```bash
# Verificar logs da inicializa√ß√£o
docker compose logs init

# Verificar se o banco existe
docker compose exec dbt-run ls -lh /app/lakehouse/

# Re-executar inicializa√ß√£o e dbt
docker compose up init
docker compose up dbt-run
```

### Problema: Scripts Python falham

**Sintomas**: Erros ao executar scripts de inicializa√ß√£o

**Solu√ß√£o**:
```bash
# Verificar logs detalhados
docker compose logs init

# Executar script manualmente para debug
docker compose exec duckdb python /app/scripts/generate_fake_data.py
docker compose exec duckdb python /app/scripts/create_real_iceberg_table.py

# Verificar depend√™ncias
docker compose exec duckdb pip list
```

### Problema: dbt run falha

**Sintomas**: Erro ao executar `dbt run`

**Solu√ß√£o**:
```bash
# Verificar logs
docker compose logs dbt-run

# Executar dbt manualmente para debug
docker compose exec dbt dbt debug
docker compose exec dbt dbt run --select staging
docker compose exec dbt dbt run --select marts

# Verificar se o banco est√° acess√≠vel
docker compose exec dbt python -c "import duckdb; con = duckdb.connect('/app/lakehouse/lakehouse.duckdb'); print(con.execute('SELECT COUNT(*) FROM vendas_iceberg').fetchone())"
```

### Problema: Containers param imediatamente

**Sintomas**: Containers iniciam e param logo em seguida

**Solu√ß√£o**:
```bash
# Verificar logs de todos os servi√ßos
docker compose logs

# Verificar status
docker compose ps -a

# Recriar tudo do zero
docker compose down -v
docker compose build --no-cache
docker compose up
```

### Limpar tudo e come√ßar do zero

Se nada funcionar, limpe tudo e recomece:

```bash
# Parar e remover tudo
docker compose down -v

# Remover imagens (opcional)
docker compose down --rmi all

# Reconstruir e iniciar
docker compose build --no-cache
docker compose up
```

## üìù Pr√≥ximos Passos

Ideias para expandir o projeto:

- [ ] Adicionar mais tabelas (clientes, produtos separados)
- [ ] Implementar streaming de dados
- [ ] Adicionar testes automatizados
- [ ] Criar dashboards (Grafana/Metabase)
- [ ] Implementar CI/CD
- [ ] Adicionar Airflow para orquestra√ß√£o
- [ ] Implementar data quality checks

## üìÑ Licen√ßa

Este projeto √© open source e est√° dispon√≠vel para fins educacionais.

## ü§ù Contribuindo

Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para abrir issues ou pull requests.

## üìö Refer√™ncias

### Documenta√ß√£o Externa

- [Apache Iceberg](https://iceberg.apache.org/)
- [DuckDB](https://duckdb.org/)
- [MinIO](https://min.io/)
- [dbt](https://www.getdbt.com/)
- [Databricks Lakehouse](https://www.databricks.com/product/data-lakehouse)

### Documenta√ß√£o do Projeto

- [Implementa√ß√£o Apache Iceberg](scripts/ICEBERG_IMPLEMENTATION.md) - Detalhes t√©cnicos sobre a implementa√ß√£o do Iceberg neste projeto

---

**Desenvolvido com ‚ù§Ô∏è para aprendizado de engenharia de dados modernos**
